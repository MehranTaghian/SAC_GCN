TODO are as follows:

1) Design the state of the environment based on the previous observation (the original fetch reach)

2) add achieved goal and desired goal as global features. Therefore, we should change the model of Q-function
and remove action from global features.

3) we should double check the observation space and action space to see if they are correct.

4) attach everything to the world body

5) there is a mismatch between qpos and qvel and the graph qpos and qvel in ant env.

6) Model changed and achieved goal removed

7) Change graph-batch according to federico's code

6) change the environment and limit the range of 9th and check the relevance

8) The fill_value argument was removed in torch-scatter>=2.0.0 as it introduced a lot of problems for scatter_max.
In scatter_max, we usually want to fill missing values with zero, while we also want to allow negative output values
as a result of grouping only negative values (rather than set this to 0 by default).

If you want to restore the old behavior, you can define out before-hand and set it to your desired fill_value:

out = src.new_zeros(output_shape)
scatter_max(src, index, dim=..., out=out)


9) You should do the following:
    a) Run 10 seeds for the normal environment to see how the relevance score of each joint changes across seeds.
    b) Run 10 seeds for the faulty environment to see how the pattern of relevance changes from normal to faulty.


10) Node_features are emtpy now because of ant

11) Outgoing features added to model node_layers

12) add torso xpos as global feature (because of reward)